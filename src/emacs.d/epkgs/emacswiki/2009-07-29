Please note that the stupid [http://www.emacswiki.org/alex/2009-07-29_Best_Essays_Spam Best Essays] link spam has reached Emacs Wiki.
These guys try to hide their nefarious spam using apparently legitimate usernames, rollbacks, and links to the old revisions.
The stupid spammers don't know that the spammed revisions will expire within two weeks, so the net result is that they're
vandalising the wiki.

You might consider watching [Self:action=rc;showedit=1;rollback=1 RecentChanges including minor edits and rollbacks] if you want to help
revert this vandalism. The edits I've looked at today all seemed to come from the IP number 119.111.124.194.
-- AlexSchroeder

[new]
We've reached the end of 2009 and the stupid essay spam is still as strong as ever. Any ideas on how to handle it without inconveniencing regular users? -- Alex

[new:DrewAdams:2009-12-24 23:28 UTC]
Sorry, no idea. It sure is a PITA, though. -- DrewAdams

[new]

Why not add a custom regex edit rule for this? Most of the domains contain the word "papers", because it's a SEO keyword for them. This world (and other words like "essay") could be banned from URLs, so if a new edit contains an URL and it contains the word "papers" then it is not allowed to proceed.

The word papers is not too frequent in legitmate URLs, so this ban could be employed for a trial period without bothering legitimate users to see its effectiveness. 


[new:DrewAdams:2009-12-25 19:43 UTC]
They're really stepping up the spam attacks -- I suppose for the holidays. The latest flurry, from the "eluneart" URL, seems to be from a bot: the edit times are close together. -- DrewAdams

[new]

What's with the CAPTCHA barrier? Have they broken reCaptcha?

[new]
Captchas are known to be broken at least on blogs.

I use wp-hashcash for my blog (which I do not have time to use now...).
It seems to work well.

    http://en.wikipedia.org/wiki/Hashcash

The strategy in hashcash is to let the client with javascript compute something that takes time (and then of course include the result in what is sent back and check that it is correct).
That works well since spammers do not have the time to wait.
Do you think this can be used here too?

[new]
I think the essay and paper spam is written by actual people. It doesn't just add spam to a page – they're actively trying to hide it by producing semi-legible text. As for the gulidova.client.kv.chereda.net spam – I looked at AlexSchroederConfigPyrobombus and noticed that they spammed the page, and then reverted their own spam, probably because it wasn't being linked? They didn't revert their own spam on EmacsNiftyTricks, for example.

If you look at the list of _all_ changes (not just the latest one for each page), you'll see that gulidova.client.kv.chereda.net spammed the site on 2009-12-25 from 17:37 to 17:55 with about an edit per minute. They had lots of time. -- Alex

[new:DrewAdams:2009-12-26 19:26 UTC]
Guess you're right. But then "they had lots of time" in both senses -- time to kill, apparently. Spending 20 minutes doing that by hand, at a page per minute -- they gotta ''love'' it! -- Drew

[new]
But then I suppose some intervention on the human level is necessary.
Registration of users?
Email confirmation of edits? -- LennartBorgman

[new]
Alex, one of the things we did on another (now defunct) wiki was to set up the robots.txt to tell google to exclude history pages. That and nofollow to the history pages. It will make the linkspam worthless, which won't necessarily reduce the dumber spammers, but it will at least render them impotent. That and 2 weeks expiry seems excessive. -- RyanDavis

[new]
For dumber spammers there could be an explicit warning on the edit page that their spam is not indexed on the history pages.

[new]
Drew, the thing I fear more than spam for money is spam for fun! :D

Lennart, I guess something like that would work – but would you have liked it? I fear that new users will prefer not to edit the wiki. Such measures are my last defense.

Ryan, currently history pages have the ##<meta name="robots" content="NOINDEX,FOLLOW" />## tag, and the link to history pages have the nofollow attribute set: ##<a class="history" rel="nofollow" href="http://www.emacswiki.org/emacs?action=history;id=2009-07-29">View other revisions</a>##. As far as I can tell, the link spam is already rendered impotent by our quick reactions. As soon as they disappear into the history pages, their edits are worth nil.

I'm trying to imagine an economic incentive that keeps their broken behaviour going. I'm thinking that poor paid-by-the-hour or paid-by-the-number-of-pages spam workers spread the spam. As far as they are concerned, their work is done as soon as their supervisor has seen it. Nobody except the spam _client_ cares whether the spam link remain active or not.

As a general rule, I also suspect that most of these edits from the point of view of a spam worker target abandoned wikis. Occasionally a wiki like ours fights back, but why should they care?

To disrupt the economics, we would need something like the following:
when there are ten edits by a particular username in fifteen minutes, adding the same link, to all of these pages, all ten edits are reverted and the domain (!) is banned. How many false positives would this generate?

-- AlexSchroeder
